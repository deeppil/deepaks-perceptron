# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DeWhMhp9wOp-yeGNAWMhZzlnbCc64X-x
"""

import numpy as np
import pandas as pd
from sklearn.utils import shuffle


class Perceptron(object):
  def __init__(self, eta=0.01, n_iter=10):
    self.eta = eta # eta is learning rate, usually float from 0 to 1(defines how quickly model learns)
    self.n_iter = n_iter #n_iter is number of iterations we run

  def weighted_sum(self, X):
    return np.dot(X, self.w_[1:]) + self.w_[0] #Summation of xi terms multiplied with wi terms(weights) this added with w_[0](bias) will give us the weighted sum.

  def predict(self, X):
    return np.where(self.weighted_sum(X) >= 0.0, 1, -1) # Step function, to determine if weighted sum is greater than 0

  # Until here is simple, we define the attributes of the Perceptron object, namely the learning rate(eta) and number of iterations n_iter. Next we create the training method.


  def fit(self, X, y):
    self.w_ = np.zeros(1 + X.shape[1]) # Setting all weight values to 0, X.shape[1] gives number of values if 1D array and number of columnds if 2D array(+1 to account for bias)
    self.errors = []

    for _ in range(self.n_iter): #iterate n_iter number of times
      error = 0 # for each iteration, we set a variable error equal to null.
      for xi ,yi in zip(X,y): #iterate through each xi, zip(X,y) relates the features/inputs(X) with the corresponding labels(y)
        y_pred = self.predict(xi) #Run the input through step function, return
        update = self.eta * (yi - y_pred) # Calculation of weight change for this iteration(if y = y_pred then no change required because then y - y_pred = 0)
        self.w_[1:] = self.w_[1:] + update * xi # Updating weights with weight change(we multiply with xi because each weight is associated with the specific input feature, hence the influence of the weight change should be prop to the input feature)
        self.w_[0] = self.w_[0] + update # Updating bias
        error += int(update != 0.0)

      self.errors.append(error)
    return self

import pandas as pd
from sklearn.utils import shuffle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib as mp


df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None) # extracting data from iris dataset
df = shuffle(df)

X = df.iloc[:, 0:4].values # X will be data used, in this case is length and width of sepal and petal for respective iris species
y = df.iloc[:,4].values # label will be type of iris species

train_data, test_data, train_labels, test_labels = train_test_split(X,y, test_size=0.25) # splitting dataset into test and train values, 25% for testing and 75% for training.

train_labels = np.where(train_labels == 'Iris-setosa', 1, -1) # Model will run to identify Iris-setosa species
test_labels = np.where(test_labels == 'Iris-setosa', 1, -1)

perceptron = Perceptron(eta=0.1,n_iter=10) # training model
perceptron.fit(train_data, train_labels)

test_preds = perceptron.predict(test_data) # testing model
accuracy = accuracy_score(test_preds, test_labels)
print('Accuracy: ', round(accuracy,2)*100, '%')

import matplotlib.pyplot as plt

plt.plot(range(1, len(errors) + 1), errors, marker='o')

plt.xlabel('Epochs')
plt.ylabel('Number of Errors')
plt.title('Perceptron Training Errors per Epoch')

# Display the plot
plt.show()

errors = perceptron.errors
print(errors)